{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MultiREx version 0.3.2\n"
     ]
    }
   ],
   "source": [
    "import multirex as mrex\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "def remove_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # Ignore warnings from the custom attributes in pandas\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        category=UserWarning,\n",
    "        message=\"Pandas doesn't allow columns to be created via a new attribute name*\",\n",
    "    )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initial setup\n",
    "remove_warnings()\n",
    "waves = np.loadtxt(\"waves.txt\")\n",
    "n_points = len(waves)\n",
    "indices = np.linspace(0, len(waves) - 1, n_points, endpoint=True)\n",
    "indices = np.round(indices).astype(int)  # Redondear los índices y convertir a entero\n",
    "\n",
    "# Seleccionar los elementos de la lista usando los índices\n",
    "puntos_seleccionados = waves[indices]\n",
    "waves = puntos_seleccionados\n",
    "wn_grid = np.sort((10000 / waves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_contaminations_from_files(contamination_files, df, n_points):\n",
    "    \"\"\"\n",
    "    Applies multiple contaminations to the data from a list of contamination files\n",
    "    and returns a DataFrame with all combinations, including the non-contaminated case.\n",
    "\n",
    "    Parameters:\n",
    "        contamination_files (list of str): Paths to .txt files containing contaminations.\n",
    "        df (pandas.DataFrame): Original DataFrame to apply contaminations.\n",
    "        n_points (int): Number of columns to which the contamination will be applied.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with all combinations of contaminations, including the\n",
    "        non-contaminated case, with additional columns 'f_spot' and 'f_fac'.\n",
    "    \"\"\"\n",
    "\n",
    "    # This version is optimized to prevent DataFrame fragmentation.\n",
    "\n",
    "    df_list = []\n",
    "    # Non-contaminated case: create a copy and add f_spot and f_fac as 0.0\n",
    "    df_no_contam = df.copy()\n",
    "    # Use assign to create new columns efficiently\n",
    "    df_no_contam = df_no_contam.assign(f_spot=0.0, f_fac=0.0)\n",
    "    # Reorder\n",
    "    cols = [\"f_spot\", \"f_fac\"] + [\n",
    "        col for col in df.columns if col not in [\"f_spot\", \"f_fac\"]\n",
    "    ]\n",
    "    df_no_contam = df_no_contam[cols]\n",
    "    df_list.append(df_no_contam)\n",
    "\n",
    "    pattern = r\"fspot(?P<f_spot>[0-9.]+)_ffac(?P<f_fac>[0-9.]+)\\.txt$\"\n",
    "\n",
    "    for file_path in contamination_files:\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = re.search(pattern, filename)\n",
    "        if not match:\n",
    "            raise ValueError(\n",
    "                f\"The file name '{filename}' does not match the expected pattern.\"\n",
    "            )\n",
    "\n",
    "        f_spot = float(match.group(\"f_spot\"))\n",
    "        f_fac = float(match.group(\"f_fac\"))\n",
    "\n",
    "        try:\n",
    "            contamination_data = np.loadtxt(file_path, ndmin=2)\n",
    "            contam_values = (\n",
    "                contamination_data[:, 1]\n",
    "                if contamination_data.shape[1] >= 2\n",
    "                else contamination_data.flatten()\n",
    "            )\n",
    "            if len(contam_values) != n_points:\n",
    "                raise ValueError(\n",
    "                    f\"Contamination values in '{filename}' ({len(contam_values)}) != n_points ({n_points}).\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading the file {file_path}: {e}\")\n",
    "\n",
    "        contam_values = contam_values[::-1]\n",
    "\n",
    "        df_contam = df.copy()\n",
    "        data_columns = df_contam.columns[-n_points:]\n",
    "\n",
    "        # Perform multiplication\n",
    "        df_contam[data_columns] = df_contam[data_columns].multiply(\n",
    "            contam_values, axis=1\n",
    "        )\n",
    "\n",
    "        # Create new columns efficiently using assign\n",
    "        df_contam = df_contam.assign(f_spot=f_spot, f_fac=f_fac)\n",
    "\n",
    "        # Reorder columns\n",
    "        cols = [\"f_spot\", \"f_fac\"] + [\n",
    "            col for col in df.columns if col not in [\"f_spot\", \"f_fac\"]\n",
    "        ]\n",
    "        df_contam = df_contam[cols]\n",
    "        df_list.append(df_contam)\n",
    "\n",
    "    df_final = pd.concat(\n",
    "        df_list, ignore_index=True\n",
    "    ).copy()  # .copy() ensures a de-fragmented frame\n",
    "    df_final.data = df_final.iloc[:, -n_points:]\n",
    "    df_final.params = df_final.iloc[:, :-n_points]\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n",
      "/tmp/ipykernel_421963/2041531900.py:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_spot\"] = f_spot\n",
      "/tmp/ipykernel_421963/2041531900.py:78: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_contam[\"f_fac\"] = f_fac\n"
     ]
    }
   ],
   "source": [
    "contamination_files = [\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.70.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.70.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.70.txt\",\n",
    "]\n",
    "\n",
    "\n",
    "def filter_rows(df):\n",
    "    \"\"\"\n",
    "    Filters rows of a DataFrame where at least one of the columns\n",
    "    \"atm CH4\", \"atm O3\", or \"atm H2O\" has a value >= -8.\n",
    "    Returns the DataFrame unchanged if none of these columns are present.\n",
    "    \"\"\"\n",
    "    filter_columns = [\"atm CH4\", \"atm O3\", \"atm H2O\"]\n",
    "    present_columns = [col for col in filter_columns if col in df.columns]\n",
    "\n",
    "    for chem in present_columns:\n",
    "        df = df[df[chem] >= -8]\n",
    "        # Set .data and .params attributes on the final DataFrame\n",
    "    df.data = df.iloc[:, -n_points:]\n",
    "    df.params = df.iloc[:, :-n_points]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Helper function to load data and correctly set dtypes\n",
    "def load_and_prep_data(filepath, n_points):\n",
    "    \"\"\"Reads a CSV and converts only the spectral data columns to float32.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Identify spectral data columns (last n_points)\n",
    "    data_cols = df.columns[-n_points:]\n",
    "    # Convert only spectral data to float32, leave params as they are\n",
    "    df[data_cols] = df[data_cols].astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "\n",
    "try:\n",
    "    airless_data = load_and_prep_data(\"spec_data/airless_data.csv\", n_points)\n",
    "    airless_data = apply_contaminations_from_files(\n",
    "        contamination_files, airless_data, n_points\n",
    "    )\n",
    "\n",
    "    CO2_data = load_and_prep_data(\"spec_data/CO2_data.csv\", n_points)\n",
    "    CO2_data = apply_contaminations_from_files(contamination_files, CO2_data, n_points)\n",
    "\n",
    "    CH4_data = load_and_prep_data(\"spec_data/CH4_data.csv\", n_points)\n",
    "    CH4_data = filter_rows(CH4_data)\n",
    "    CH4_data = apply_contaminations_from_files(contamination_files, CH4_data, n_points)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing initial data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_df(df, n_points, n_mult):\n",
    "    df_list = []\n",
    "    for _ in range(n_mult + 1):\n",
    "        df_no_contam = df.copy()\n",
    "        # Use assign to avoid fragmentation\n",
    "        df_no_contam = df_no_contam.assign(f_spot=0.0, f_fac=0.0)\n",
    "        cols = [\"f_spot\", \"f_fac\"] + [\n",
    "            col for col in df.columns if col not in [\"f_spot\", \"f_fac\"]\n",
    "        ]\n",
    "        df_no_contam = df_no_contam[cols]\n",
    "        df_list.append(df_no_contam)\n",
    "\n",
    "    df_final = pd.concat(df_list, ignore_index=True).copy()  # .copy() de-fragments\n",
    "    df_final.data = df_final.iloc[:, -n_points:]\n",
    "    df_final.params = df_final.iloc[:, :-n_points]\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    airless_data_clean = load_and_prep_data(\"spec_data/airless_data.csv\", n_points)\n",
    "    airless_data_clean = mult_df(airless_data_clean, n_points, 9)\n",
    "\n",
    "    CO2_data_clean = load_and_prep_data(\"spec_data/CO2_data.csv\", n_points)\n",
    "    CO2_data_clean = mult_df(CO2_data_clean, n_points, 9)\n",
    "\n",
    "    CH4_data_clean = load_and_prep_data(\"spec_data/CH4_data.csv\", n_points)\n",
    "    CH4_data_clean = filter_rows(CH4_data_clean)\n",
    "    CH4_data_clean = mult_df(CH4_data_clean, n_points, 9)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing clean data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max_by_row(df):\n",
    "    min_by_row = df.min(axis=1)\n",
    "    max_by_row = df.max(axis=1)\n",
    "    range_by_row = max_by_row - min_by_row\n",
    "    # Avoid division by zero\n",
    "    range_by_row[range_by_row == 0] = 1\n",
    "    normalized = (df.sub(min_by_row, axis=0)).div(range_by_row, axis=0)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy and Clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max_by_row(df):\n",
    "    min_by_row = df.min(axis=1)\n",
    "    max_by_row = df.max(axis=1)\n",
    "    range_by_row = max_by_row - min_by_row\n",
    "    # Avoid division by zero\n",
    "    range_by_row[range_by_row == 0] = 1\n",
    "    normalized = (df.sub(min_by_row, axis=0)).div(range_by_row, axis=0)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def generate_df_with_noise_std(df, n_repeat, noise_std, seed=None):\n",
    "    if not hasattr(df, \"params\"):\n",
    "        df_params = pd.DataFrame()\n",
    "        if not hasattr(df, \"data\"):\n",
    "            df_spectra = df\n",
    "    else:\n",
    "        if not hasattr(df, \"data\"):\n",
    "            raise ValueError(\"The DataFrame must have a 'data' attribute.\")\n",
    "        df_params = df.params\n",
    "        df_spectra = df.data\n",
    "\n",
    "    df_spectra = df_spectra.astype(\"float32\")\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    df_spectra_replicated = pd.DataFrame(\n",
    "        np.repeat(df_spectra.values, n_repeat, axis=0),\n",
    "        columns=df_spectra.columns,\n",
    "    )\n",
    "\n",
    "    if isinstance(noise_std, (int, float)):\n",
    "        noise_replicated = np.full(\n",
    "            df_spectra_replicated.shape, noise_std, dtype=\"float32\"\n",
    "        )\n",
    "    else:\n",
    "        noise_array = np.array(noise_std, dtype=\"float32\")\n",
    "        noise_replicated = np.repeat(noise_array[:, np.newaxis], n_repeat, axis=0)\n",
    "        noise_replicated = np.tile(\n",
    "            noise_replicated, (1, df_spectra_replicated.shape[1])\n",
    "        )\n",
    "\n",
    "    gaussian_noise = np.random.normal(\n",
    "        0, noise_replicated, df_spectra_replicated.shape\n",
    "    ).astype(\"float32\")\n",
    "    df_spectra_replicated += gaussian_noise\n",
    "\n",
    "    df_params_replicated = pd.DataFrame(\n",
    "        np.repeat(df_params.values, n_repeat, axis=0),\n",
    "        columns=df_params.columns,\n",
    "    )\n",
    "\n",
    "    # Efficiently add new columns\n",
    "    new_cols_data = {\n",
    "        \"noise_std\": (\n",
    "            np.repeat(noise_std, n_repeat)\n",
    "            if isinstance(noise_std, (list, np.ndarray, pd.Series))\n",
    "            else noise_std\n",
    "        ),\n",
    "        \"n_repeat\": n_repeat,\n",
    "    }\n",
    "    new_cols_df = pd.DataFrame(new_cols_data, index=df_params_replicated.index)\n",
    "\n",
    "    # Concatenate all parts at once\n",
    "    df_final = pd.concat(\n",
    "        [\n",
    "            new_cols_df,\n",
    "            df_params_replicated.reset_index(drop=True),\n",
    "            df_spectra_replicated.reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_final.data = df_final.iloc[:, -df_spectra_replicated.shape[1] :]\n",
    "    df_final.params = df_final.iloc[:, : -df_spectra_replicated.shape[1]]\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMIZED: Generate and Process All Data\n",
    "\n",
    "This single cell replaces all the individual SNR cells and the final `## Data` cell.\n",
    "It loops through each SNR value, generates the noisy and clean data, normalizes it,\n",
    "appends the result to a list as a NumPy array, and then clears the memory.\n",
    "This avoids holding multiple massive dataframes in memory at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = mrex.generate_df_SNR_noise(df=CO2_data, n_repeat=1, SNR=1)[\"noise\"][1]\n",
    "\n",
    "# SNR1_CO2_data = generate_df_with_noise_std(df=CO2_data, n_repeat=5_000, noise_std=noise)\n",
    "# SNR1_CH4_data = generate_df_with_noise_std(df=CH4_data, n_repeat=500, noise_std=noise)\n",
    "# SNR1_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data, n_repeat=5_000, noise_std=noise\n",
    "# )\n",
    "\n",
    "# SNR1_df = pd.concat(\n",
    "#     [SNR1_CO2_data, SNR1_CH4_data, SNR1_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNR1_CO2_data, SNR1_CH4_data, SNR1_airless_data)\n",
    "# gc.collect()\n",
    "\n",
    "# ##-------------\n",
    "# SNR1_no_CO2_data = generate_df_with_noise_std(\n",
    "#     df=CO2_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "# SNR1_no_CH4_data = generate_df_with_noise_std(\n",
    "#     df=CH4_data_clean, n_repeat=500, noise_std=0\n",
    "# )\n",
    "# SNR1_no_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNR1_no_df = pd.concat(\n",
    "#     [SNR1_no_CO2_data, SNR1_no_CH4_data, SNR1_no_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNR1_no_CO2_data, SNR1_no_CH4_data, SNR1_no_airless_data)\n",
    "# gc.collect()\n",
    "\n",
    "# ##-------------\n",
    "# print(\n",
    "#     \"Clean and noise dataframes have the same shape: \",\n",
    "#     SNR1_df.shape == SNR1_no_df.shape,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = mrex.generate_df_SNR_noise(df=CO2_data, n_repeat=1, SNR=3)[\"noise\"][1]\n",
    "\n",
    "# SNR3_CO2_data = generate_df_with_noise_std(df=CO2_data, n_repeat=5_000, noise_std=noise)\n",
    "# SNR3_CH4_data = generate_df_with_noise_std(df=CH4_data, n_repeat=500, noise_std=noise)\n",
    "# SNR3_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data, n_repeat=5_000, noise_std=noise\n",
    "# )\n",
    "\n",
    "# SNR3_df = pd.concat(\n",
    "#     [SNR3_CO2_data, SNR3_CH4_data, SNR3_airless_data], ignore_index=True\n",
    "# )\n",
    "# del (SNR3_CO2_data, SNR3_CH4_data, SNR3_airless_data)\n",
    "\n",
    "# # -------------------------------\n",
    "# SNR3_no_CO2_data = generate_df_with_noise_std(\n",
    "#     df=CO2_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNR3_no_CH4_data = generate_df_with_noise_std(\n",
    "#     df=CH4_data_clean, n_repeat=500, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNR3_no_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNR3_no_df = pd.concat(\n",
    "#     [SNR3_no_CO2_data, SNR3_no_CH4_data, SNR3_no_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNR3_no_CO2_data, SNR3_no_CH4_data, SNR3_no_airless_data)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# ##-------------\n",
    "# print(\n",
    "#     \"Clean and noise dataframes have the same shape: \",\n",
    "#     SNR3_df.shape == SNR3_no_df.shape,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR = 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = mrex.generate_df_SNR_noise(df=CO2_data, n_repeat=1, SNR=6)[\"noise\"][1]\n",
    "\n",
    "# SNR6_CO2_data = generate_df_with_noise_std(df=CO2_data, n_repeat=5_000, noise_std=noise)\n",
    "# SNR6_CH4_data = generate_df_with_noise_std(df=CH4_data, n_repeat=500, noise_std=noise)\n",
    "# SNR6_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data, n_repeat=5_000, noise_std=noise\n",
    "# )\n",
    "\n",
    "# SNR6_df = pd.concat(\n",
    "#     [SNR6_CO2_data, SNR6_CH4_data, SNR6_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNR6_CO2_data, SNR6_CH4_data, SNR6_airless_data)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# # -------------------------------\n",
    "# SNR6_no_CO2_data = generate_df_with_noise_std(\n",
    "#     df=CO2_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "# SNR6_no_CH4_data = generate_df_with_noise_std(\n",
    "#     df=CH4_data_clean, n_repeat=500, noise_std=0\n",
    "# )\n",
    "# SNR6_no_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "# SNR6_no_df = pd.concat(\n",
    "#     [SNR6_no_CO2_data, SNR6_no_CH4_data, SNR6_no_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNR6_no_CO2_data, SNR6_no_CH4_data, SNR6_no_airless_data)\n",
    "# gc.collect()\n",
    "\n",
    "# ##-------------\n",
    "# print(\n",
    "#     \"Clean and noise dataframes have the same shape: \",\n",
    "#     SNR6_df.shape == SNR6_no_df.shape,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise = mrex.generate_df_SNR_noise(df=CO2_data, n_repeat=1, SNR=10)[\"noise\"][1]\n",
    "\n",
    "# SNR10_CO2_data = generate_df_with_noise_std(\n",
    "#     df=CO2_data, n_repeat=5_000, noise_std=noise\n",
    "# )\n",
    "\n",
    "# SNR10_CH4_data = generate_df_with_noise_std(df=CH4_data, n_repeat=500, noise_std=noise)\n",
    "\n",
    "# SNR10_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data, n_repeat=5_000, noise_std=noise\n",
    "# )\n",
    "\n",
    "# SNR10_df = pd.concat(\n",
    "#     [SNR10_CO2_data, SNR10_CH4_data, SNR10_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "\n",
    "# del (SNR10_CO2_data, SNR10_CH4_data, SNR10_airless_data)\n",
    "\n",
    "# # -------------------------------\n",
    "# SNR10_no_CO2_data = generate_df_with_noise_std(\n",
    "#     df=CO2_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNR10_no_CH4_data = generate_df_with_noise_std(\n",
    "#     df=CH4_data_clean, n_repeat=500, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNR10_no_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNR10_no_df = pd.concat(\n",
    "#     [SNR10_no_CO2_data, SNR10_no_CH4_data, SNR10_no_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNR10_no_CO2_data, SNR10_no_CH4_data, SNR10_no_airless_data)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# ##-------------\n",
    "# print(\n",
    "#     \"Clean and noise dataframes have the same shape: \",\n",
    "#     SNR10_df.shape == SNR10_no_df.shape,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SNR = Nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNRNan_CO2_data = generate_df_with_noise_std(df=CO2_data, n_repeat=5_000, noise_std=0)\n",
    "# SNRNan_CH4_data = generate_df_with_noise_std(df=CH4_data, n_repeat=500, noise_std=0)\n",
    "# SNRNan_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNRNan_df = pd.concat(\n",
    "#     [SNRNan_CO2_data, SNRNan_CH4_data, SNRNan_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNRNan_CO2_data, SNRNan_CH4_data, SNRNan_airless_data)\n",
    "\n",
    "# # -------------------------------\n",
    "# SNRNan_no_CO2_data = generate_df_with_noise_std(\n",
    "#     df=CO2_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "# SNRNan_no_CH4_data = generate_df_with_noise_std(\n",
    "#     df=CH4_data_clean, n_repeat=500, noise_std=0\n",
    "# )\n",
    "# SNRNan_no_airless_data = generate_df_with_noise_std(\n",
    "#     df=airless_data_clean, n_repeat=5_000, noise_std=0\n",
    "# )\n",
    "\n",
    "# SNRNan_no_df = pd.concat(\n",
    "#     [SNRNan_no_CO2_data, SNRNan_no_CH4_data, SNRNan_no_airless_data], ignore_index=True\n",
    "# )\n",
    "\n",
    "# del (SNRNan_no_CO2_data, SNRNan_no_CH4_data, SNRNan_no_airless_data)\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# ##-------------\n",
    "# print(\n",
    "#     \"Clean and noise dataframes have the same shape: \",\n",
    "#     SNRNan_df.shape == SNRNan_no_df.shape,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Noisy data\n",
    "# ##-------------\n",
    "# # Concatenate noisy DataFrames and convert labels from string to list\n",
    "# SNRall_noisy = pd.concat(\n",
    "#     [SNR1_df, SNR3_df, SNR6_df, SNR10_df, SNRNan_df], ignore_index=True\n",
    "# )\n",
    "\n",
    "# # Normalize the last n_points columns row-wise\n",
    "# SNRall_noisy.iloc[:, -n_points:] = normalize_min_max_by_row(\n",
    "#     SNRall_noisy.iloc[:, -n_points:]\n",
    "# )\n",
    "# X_noisy = SNRall_noisy.iloc[:, -n_points:].values\n",
    "\n",
    "# ##-------------\n",
    "# # Clean data\n",
    "# # Concatenate non-noisy DataFrames and convert labels\n",
    "# SNRall_no_noisy = pd.concat(\n",
    "#     [SNR1_no_df, SNR3_no_df, SNR6_no_df, SNR10_no_df, SNRNan_no_df], ignore_index=True\n",
    "# )\n",
    "# # Normalize the last n_points columns row-wise\n",
    "# SNRall_no_noisy.iloc[:, -n_points:] = normalize_min_max_by_row(\n",
    "#     SNRall_no_noisy.iloc[:, -n_points:]\n",
    "# )\n",
    "# X_no_noisy = SNRall_no_noisy.iloc[:, -n_points:].values\n",
    "\n",
    "# # Ensure the number of samples match\n",
    "# assert (\n",
    "#     SNRall_noisy.shape[0] == SNRall_no_noisy.shape[0]\n",
    "# ), \"The number of samples does not match between X_noisy and X_clean.\"\n",
    "\n",
    "# # Clean up variables and force garbage collection\n",
    "# del SNRall_noisy, SNRall_no_noisy\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_noisy_arrays = []\n",
    "list_of_clean_arrays = []\n",
    "\n",
    "# Using None for the 'Nan' (no noise) case\n",
    "snr_values = [1, 3, 6, 10, None]\n",
    "\n",
    "for snr in snr_values:\n",
    "    print(\n",
    "        f\"--- Processing SNR = {snr if snr is not None else 'inf (no additional noise)'} ---\"\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Generate Noisy Data (Contaminated + Noise) ---\n",
    "    if snr is not None:\n",
    "        noise = mrex.generate_df_SNR_noise(df=CO2_data, n_repeat=1, SNR=snr)[\"noise\"][0]\n",
    "    else:\n",
    "        noise = 0.0  # No additional noise for the \"Nan\" case\n",
    "\n",
    "    temp_CO2_data = generate_df_with_noise_std(\n",
    "        df=CO2_data, n_repeat=5000, noise_std=noise\n",
    "    )\n",
    "    temp_CH4_data = generate_df_with_noise_std(\n",
    "        df=CH4_data, n_repeat=500, noise_std=noise\n",
    "    )\n",
    "    temp_airless_data = generate_df_with_noise_std(\n",
    "        df=airless_data, n_repeat=5000, noise_std=noise\n",
    "    )\n",
    "\n",
    "    current_noisy_df = pd.concat(\n",
    "        [temp_CO2_data, temp_CH4_data, temp_airless_data], ignore_index=True\n",
    "    )\n",
    "\n",
    "    normalized_data = normalize_min_max_by_row(current_noisy_df.iloc[:, -n_points:])\n",
    "    list_of_noisy_arrays.append(normalized_data.values.astype(\"float32\"))\n",
    "\n",
    "    del (\n",
    "        temp_CO2_data,\n",
    "        temp_CH4_data,\n",
    "        temp_airless_data,\n",
    "        current_noisy_df,\n",
    "        normalized_data,\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Generate Corresponding Clean Data (Uncontaminated + No Noise) ---\n",
    "    temp_no_CO2_data = generate_df_with_noise_std(\n",
    "        df=CO2_data_clean, n_repeat=5000, noise_std=0\n",
    "    )\n",
    "    temp_no_CH4_data = generate_df_with_noise_std(\n",
    "        df=CH4_data_clean, n_repeat=500, noise_std=0\n",
    "    )\n",
    "    temp_no_airless_data = generate_df_with_noise_std(\n",
    "        df=airless_data_clean, n_repeat=5000, noise_std=0\n",
    "    )\n",
    "\n",
    "    current_clean_df = pd.concat(\n",
    "        [temp_no_CO2_data, temp_no_CH4_data, temp_no_airless_data], ignore_index=True\n",
    "    )\n",
    "\n",
    "    normalized_data = normalize_min_max_by_row(current_clean_df.iloc[:, -n_points:])\n",
    "    list_of_clean_arrays.append(normalized_data.values.astype(\"float32\"))\n",
    "\n",
    "    del (\n",
    "        temp_no_CO2_data,\n",
    "        temp_no_CH4_data,\n",
    "        temp_no_airless_data,\n",
    "        current_clean_df,\n",
    "        normalized_data,\n",
    "    )\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n--- Final Data Concatenation ---\")\n",
    "X_noisy = np.concatenate(list_of_noisy_arrays, axis=0)\n",
    "del list_of_noisy_arrays\n",
    "\n",
    "X_no_noisy = np.concatenate(list_of_clean_arrays, axis=0)\n",
    "del list_of_clean_arrays\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Final noisy data shape: {X_noisy.shape}\")\n",
    "print(f\"Final clean data shape: {X_no_noisy.shape}\")\n",
    "assert X_noisy.shape[0] == X_no_noisy.shape[0], \"The number of samples does not match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FInal data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data prep and Model Training\n",
    "\n",
    "The rest of your script remains the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "X_train_noisy, X_test_noisy, X_train_clean, X_test_clean = train_test_split(\n",
    "    X_noisy, X_no_noisy, test_size=test_size, random_state=42\n",
    ")\n",
    "del (X_noisy, X_no_noisy)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_noisy.shape[1]\n",
    "\n",
    "input_spectrum = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoded = layers.Dense(512, activation=\"swish\")(input_spectrum)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(512, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(512, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(300, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(input_dim, activation=\"linear\")(decoded)\n",
    "\n",
    "autoencoder = keras.Model(inputs=input_spectrum, outputs=decoded)\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "autoencoder.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "history = autoencoder.fit(\n",
    "    X_train_noisy,\n",
    "    X_train_clean,\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test_noisy, X_test_clean),\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "autoencoder.save(\"AE_CH4.keras\")\n",
    "\n",
    "# Plot training and validation MAE\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history[\"loss\"], label=\"Training MAE\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation MAE\")\n",
    "plt.title(\"MAE Progress During Training\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MAE\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Predict reconstructed spectra on test data\n",
    "decoded_spectra = autoencoder.predict(X_test_noisy)\n",
    "\n",
    "# Visualize a few reconstructions\n",
    "num_samples = 5  # Number of samples to visualize\n",
    "indices = np.random.choice(len(X_test_noisy), num_samples, replace=False)\n",
    "\n",
    "for idx in indices:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(waves, X_test_clean[idx].flatten(), label=\"Original Clean Spectrum\")\n",
    "    plt.plot(\n",
    "        waves, X_test_noisy[idx].flatten(), label=\"Noisy Input Spectrum\", alpha=0.5\n",
    "    )\n",
    "    plt.plot(\n",
    "        waves,\n",
    "        decoded_spectra[idx].flatten(),\n",
    "        label=\"Denoised (Reconstructed) Spectrum\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    plt.xlabel(\"Wavelength\")\n",
    "    plt.ylabel(\"Normalized Intensity\")\n",
    "    plt.title(f\"Spectrum Reconstruction - Sample {idx}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Re-load the best model saved by EarlyStopping\n",
    "autoencoder = keras.models.load_model(\"AE_CH4.keras\")\n",
    "X_reconstructed = autoencoder.predict(X_test_noisy)\n",
    "\n",
    "# Compute evaluation metrics on the test set\n",
    "mae = mean_absolute_error(X_test_clean, X_reconstructed)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "\n",
    "mse = mean_squared_error(X_test_clean, X_reconstructed)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
    "\n",
    "r2 = r2_score(X_test_clean.flatten(), X_reconstructed.flatten())\n",
    "print(f\"Coefficient of Determination (R²): {r2:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

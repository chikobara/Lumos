{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiREx Autoencoder for Exoplanet Spectra Denoising (CH4)\n",
    "\n",
    "This script trains a deep learning model to remove noise and stellar contamination\n",
    "from exoplanet transit spectra. This version is fully optimized for memory efficiency\n",
    "on both CPU and GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numba not installed, using numpy instead\n",
      "Loading MultiREx version 0.3.2\n"
     ]
    }
   ],
   "source": [
    "import multirex as mrex\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def remove_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    # Ignore warnings from the custom attributes in pandas\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        category=UserWarning,\n",
    "        message=\"Pandas doesn't allow columns to be created via a new attribute name*\",\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Initial setup\n",
    "remove_warnings()\n",
    "waves = np.loadtxt(\"waves.txt\")\n",
    "n_points = len(waves)\n",
    "indices = np.linspace(0, len(waves) - 1, n_points, endpoint=True)\n",
    "indices = np.round(indices).astype(int)  # Redondear los índices y convertir a entero\n",
    "\n",
    "# Seleccionar los elementos de la lista usando los índices\n",
    "puntos_seleccionados = waves[indices]\n",
    "waves = puntos_seleccionados\n",
    "wn_grid = np.sort((10000 / waves))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_contaminations_from_files(contamination_files, df, n_points):\n",
    "    \"\"\"\n",
    "    Applies multiple contaminations to the data from a list of contamination files\n",
    "    and returns a DataFrame with all combinations, including the non-contaminated case.\n",
    "\n",
    "    Parameters:\n",
    "        contamination_files (list of str): Paths to .txt files containing contaminations.\n",
    "        df (pandas.DataFrame): Original DataFrame to apply contaminations.\n",
    "        n_points (int): Number of columns to which the contamination will be applied.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with all combinations of contaminations, including the\n",
    "        non-contaminated case, with additional columns 'f_spot' and 'f_fac'.\n",
    "    \"\"\"\n",
    "\n",
    "    # This version is optimized to prevent DataFrame fragmentation.\n",
    "\n",
    "    df_list = []\n",
    "    # Non-contaminated case: create a copy and add f_spot and f_fac as 0.0\n",
    "    df_no_contam = df.copy()\n",
    "    # Use assign to create new columns efficiently\n",
    "    df_no_contam = df_no_contam.assign(f_spot=0.0, f_fac=0.0)\n",
    "    # Reorder\n",
    "    cols = [\"f_spot\", \"f_fac\"] + [\n",
    "        col for col in df.columns if col not in [\"f_spot\", \"f_fac\"]\n",
    "    ]\n",
    "    df_no_contam = df_no_contam[cols]\n",
    "    df_list.append(df_no_contam)\n",
    "\n",
    "    pattern = r\"fspot(?P<f_spot>[0-9.]+)_ffac(?P<f_fac>[0-9.]+)\\.txt$\"\n",
    "\n",
    "    for file_path in contamination_files:\n",
    "        if not os.path.isfile(file_path):\n",
    "            raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "\n",
    "        filename = os.path.basename(file_path)\n",
    "        match = re.search(pattern, filename)\n",
    "        if not match:\n",
    "            raise ValueError(\n",
    "                f\"The file name '{filename}' does not match the expected pattern.\"\n",
    "            )\n",
    "\n",
    "        f_spot = float(match.group(\"f_spot\"))\n",
    "        f_fac = float(match.group(\"f_fac\"))\n",
    "\n",
    "        try:\n",
    "            contamination_data = np.loadtxt(file_path, ndmin=2)\n",
    "            contam_values = (\n",
    "                contamination_data[:, 1]\n",
    "                if contamination_data.shape[1] >= 2\n",
    "                else contamination_data.flatten()\n",
    "            )\n",
    "            if len(contam_values) != n_points:\n",
    "                raise ValueError(\n",
    "                    f\"Contamination values in '{filename}' ({len(contam_values)}) != n_points ({n_points}).\"\n",
    "                )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading the file {file_path}: {e}\")\n",
    "\n",
    "        contam_values = contam_values[::-1]\n",
    "\n",
    "        df_contam = df.copy()\n",
    "        data_columns = df_contam.columns[-n_points:]\n",
    "\n",
    "        # Perform multiplication\n",
    "        df_contam[data_columns] = df_contam[data_columns].multiply(\n",
    "            contam_values, axis=1\n",
    "        )\n",
    "\n",
    "        # Create new columns efficiently using assign\n",
    "        df_contam = df_contam.assign(f_spot=f_spot, f_fac=f_fac)\n",
    "\n",
    "        # Reorder columns\n",
    "        cols = [\"f_spot\", \"f_fac\"] + [\n",
    "            col for col in df.columns if col not in [\"f_spot\", \"f_fac\"]\n",
    "        ]\n",
    "        df_contam = df_contam[cols]\n",
    "        df_list.append(df_contam)\n",
    "\n",
    "    df_final = pd.concat(\n",
    "        df_list, ignore_index=True\n",
    "    ).copy()  # .copy() ensures a de-fragmented frame\n",
    "    df_final.data = df_final.iloc[:, -n_points:]\n",
    "    df_final.params = df_final.iloc[:, :-n_points]\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination_files = [\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.70.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.70.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.70.txt\",\n",
    "]\n",
    "\n",
    "\n",
    "def filter_rows(df):\n",
    "    \"\"\"\n",
    "    Filters rows of a DataFrame where at least one of the columns\n",
    "    \"atm CH4\", \"atm O3\", or \"atm H2O\" has a value >= -8.\n",
    "    Returns the DataFrame unchanged if none of these columns are present.\n",
    "    \"\"\"\n",
    "    filter_columns = [\"atm CH4\", \"atm O3\", \"atm H2O\"]\n",
    "    present_columns = [col for col in filter_columns if col in df.columns]\n",
    "\n",
    "    for chem in present_columns:\n",
    "        df = df[df[chem] >= -8]\n",
    "        # Set .data and .params attributes on the final DataFrame\n",
    "    df.data = df.iloc[:, -n_points:]\n",
    "    df.params = df.iloc[:, :-n_points]\n",
    "    return df\n",
    "\n",
    "\n",
    "# Helper function to load data and correctly set dtypes\n",
    "def load_and_prep_data(filepath, n_points):\n",
    "    \"\"\"Reads a CSV and converts only the spectral data columns to float32.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Identify spectral data columns (last n_points)\n",
    "    data_cols = df.columns[-n_points:]\n",
    "    # Convert only spectral data to float32, leave params as they are\n",
    "    df[data_cols] = df[data_cols].astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "\n",
    "try:\n",
    "    airless_data = load_and_prep_data(\"spec_data/airless_data.csv\", n_points)\n",
    "    airless_data = apply_contaminations_from_files(\n",
    "        contamination_files, airless_data, n_points\n",
    "    )\n",
    "\n",
    "    CO2_data = load_and_prep_data(\"spec_data/CO2_data.csv\", n_points)\n",
    "    CO2_data = apply_contaminations_from_files(contamination_files, CO2_data, n_points)\n",
    "\n",
    "    CH4_data = load_and_prep_data(\"spec_data/CH4_data.csv\", n_points)\n",
    "    CH4_data = filter_rows(CH4_data)\n",
    "    CH4_data = apply_contaminations_from_files(contamination_files, CH4_data, n_points)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing initial data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_df(df, n_points, n_mult):\n",
    "    df_list = []\n",
    "    for _ in range(n_mult + 1):\n",
    "        df_no_contam = df.copy()\n",
    "        # Use assign to avoid fragmentation\n",
    "        df_no_contam = df_no_contam.assign(f_spot=0.0, f_fac=0.0)\n",
    "        cols = [\"f_spot\", \"f_fac\"] + [\n",
    "            col for col in df.columns if col not in [\"f_spot\", \"f_fac\"]\n",
    "        ]\n",
    "        df_no_contam = df_no_contam[cols]\n",
    "        df_list.append(df_no_contam)\n",
    "\n",
    "    df_final = pd.concat(df_list, ignore_index=True).copy()  # .copy() de-fragments\n",
    "    df_final.data = df_final.iloc[:, -n_points:]\n",
    "    df_final.params = df_final.iloc[:, :-n_points]\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    airless_data_clean = load_and_prep_data(\"spec_data/airless_data.csv\", n_points)\n",
    "    airless_data_clean = mult_df(airless_data_clean, n_points, 9)\n",
    "\n",
    "    CO2_data_clean = load_and_prep_data(\"spec_data/CO2_data.csv\", n_points)\n",
    "    CO2_data_clean = mult_df(CO2_data_clean, n_points, 9)\n",
    "\n",
    "    CH4_data_clean = load_and_prep_data(\"spec_data/CH4_data.csv\", n_points)\n",
    "    CH4_data_clean = filter_rows(CH4_data_clean)\n",
    "    CH4_data_clean = mult_df(CH4_data_clean, n_points, 9)\n",
    "except Exception as e:\n",
    "    print(f\"Error processing clean data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max_by_row(df):\n",
    "    min_by_row = df.min(axis=1)\n",
    "    max_by_row = df.max(axis=1)\n",
    "    range_by_row = max_by_row - min_by_row\n",
    "    # Avoid division by zero\n",
    "    range_by_row[range_by_row == 0] = 1\n",
    "    normalized = (df.sub(min_by_row, axis=0)).div(range_by_row, axis=0)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy and Clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_min_max_by_row(df):\n",
    "    min_by_row = df.min(axis=1)\n",
    "    max_by_row = df.max(axis=1)\n",
    "    range_by_row = max_by_row - min_by_row\n",
    "    # Avoid division by zero\n",
    "    range_by_row[range_by_row == 0] = 1\n",
    "    normalized = (df.sub(min_by_row, axis=0)).div(range_by_row, axis=0)\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def generate_df_with_noise_std(df, n_repeat, noise_std, seed=None):\n",
    "    if not hasattr(df, \"params\"):\n",
    "        df_params = pd.DataFrame()\n",
    "        if not hasattr(df, \"data\"):\n",
    "            df_spectra = df\n",
    "    else:\n",
    "        if not hasattr(df, \"data\"):\n",
    "            raise ValueError(\"The DataFrame must have a 'data' attribute.\")\n",
    "        df_params = df.params\n",
    "        df_spectra = df.data\n",
    "\n",
    "    df_spectra = df_spectra.astype(\"float32\")\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    df_spectra_replicated = pd.DataFrame(\n",
    "        np.repeat(df_spectra.values, n_repeat, axis=0),\n",
    "        columns=df_spectra.columns,\n",
    "    )\n",
    "\n",
    "    if isinstance(noise_std, (int, float)):\n",
    "        noise_replicated = np.full(\n",
    "            df_spectra_replicated.shape, noise_std, dtype=\"float32\"\n",
    "        )\n",
    "    else:\n",
    "        noise_array = np.array(noise_std, dtype=\"float32\")\n",
    "        noise_replicated = np.repeat(noise_array[:, np.newaxis], n_repeat, axis=0)\n",
    "        noise_replicated = np.tile(\n",
    "            noise_replicated, (1, df_spectra_replicated.shape[1])\n",
    "        )\n",
    "\n",
    "    gaussian_noise = np.random.normal(\n",
    "        0, noise_replicated, df_spectra_replicated.shape\n",
    "    ).astype(\"float32\")\n",
    "    df_spectra_replicated += gaussian_noise\n",
    "\n",
    "    df_params_replicated = pd.DataFrame(\n",
    "        np.repeat(df_params.values, n_repeat, axis=0),\n",
    "        columns=df_params.columns,\n",
    "    )\n",
    "\n",
    "    # Efficiently add new columns\n",
    "    new_cols_data = {\n",
    "        \"noise_std\": (\n",
    "            np.repeat(noise_std, n_repeat)\n",
    "            if isinstance(noise_std, (list, np.ndarray, pd.Series))\n",
    "            else noise_std\n",
    "        ),\n",
    "        \"n_repeat\": n_repeat,\n",
    "    }\n",
    "    new_cols_df = pd.DataFrame(new_cols_data, index=df_params_replicated.index)\n",
    "\n",
    "    # Concatenate all parts at once\n",
    "    df_final = pd.concat(\n",
    "        [\n",
    "            new_cols_df,\n",
    "            df_params_replicated.reset_index(drop=True),\n",
    "            df_spectra_replicated.reset_index(drop=True),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df_final.data = df_final.iloc[:, -df_spectra_replicated.shape[1] :]\n",
    "    df_final.params = df_final.iloc[:, : -df_spectra_replicated.shape[1]]\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMIZED: Generate and Process All Data\n",
    "\n",
    "This single cell replaces all the individual SNR cells and the final `## Data` cell.\n",
    "It loops through each SNR value, generates the noisy and clean data, normalizes it,\n",
    "appends the result to a list as a NumPy array, and then clears the memory.\n",
    "This avoids holding multiple massive dataframes in memory at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opt snr and noisy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached data. Loading from disk...\n",
      "...Loading complete.\n",
      "Loaded noisy data shape: (1600000, 385)\n",
      "Loaded clean data shape: (1600000, 385)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define file paths for the cached numpy arrays in a dedicated directory\n",
    "output_dir = \"processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
    "noisy_path = os.path.join(output_dir, \"CH4_noisy_full_dataset.npy\")\n",
    "clean_path = os.path.join(output_dir, \"CH4_clean_full_dataset.npy\")\n",
    "\n",
    "# Caching Logic: Check if files exist\n",
    "if os.path.exists(noisy_path) and os.path.exists(clean_path):\n",
    "    print(\"Found cached data. Loading from disk...\")\n",
    "    X_noisy = np.load(noisy_path)\n",
    "    X_no_noisy = np.load(clean_path)\n",
    "    print(\"...Loading complete.\")\n",
    "    print(f\"Loaded noisy data shape: {X_noisy.shape}\")\n",
    "    print(f\"Loaded clean data shape: {X_no_noisy.shape}\")\n",
    "\n",
    "else:\n",
    "    print(\"Cached data not found. Starting data generation process...\")\n",
    "\n",
    "    list_of_noisy_arrays = []\n",
    "    list_of_clean_arrays = []\n",
    "\n",
    "    # Using None for the 'Nan' (no noise) case\n",
    "    snr_values = [1, 3, 6, 10, None]\n",
    "\n",
    "    for snr in snr_values:\n",
    "        print(\n",
    "            f\"--- Processing SNR = {snr if snr is not None else 'inf (no additional noise)'} ---\"\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # --- Generate Noisy Data (Contaminated + Noise) ---\n",
    "        if snr is not None:\n",
    "            noise = mrex.generate_df_SNR_noise(df=CO2_data, n_repeat=1, SNR=snr)[\n",
    "                \"noise\"\n",
    "            ][0]\n",
    "        else:\n",
    "            noise = 0.0  # No additional noise for the \"Nan\" case\n",
    "\n",
    "        temp_CO2_data = generate_df_with_noise_std(\n",
    "            df=CO2_data, n_repeat=5000, noise_std=noise\n",
    "        )\n",
    "        temp_CH4_data = generate_df_with_noise_std(\n",
    "            df=CH4_data, n_repeat=500, noise_std=noise\n",
    "        )\n",
    "        temp_airless_data = generate_df_with_noise_std(\n",
    "            df=airless_data, n_repeat=5000, noise_std=noise\n",
    "        )\n",
    "\n",
    "        current_noisy_df = pd.concat(\n",
    "            [temp_CO2_data, temp_CH4_data, temp_airless_data], ignore_index=True\n",
    "        )\n",
    "\n",
    "        normalized_data = normalize_min_max_by_row(current_noisy_df.iloc[:, -n_points:])\n",
    "        list_of_noisy_arrays.append(normalized_data.values.astype(\"float32\"))\n",
    "\n",
    "        del (\n",
    "            temp_CO2_data,\n",
    "            temp_CH4_data,\n",
    "            temp_airless_data,\n",
    "            current_noisy_df,\n",
    "            normalized_data,\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "        # --- Generate Corresponding Clean Data (Uncontaminated + No Noise) ---\n",
    "        temp_no_CO2_data = generate_df_with_noise_std(\n",
    "            df=CO2_data_clean, n_repeat=5000, noise_std=0\n",
    "        )\n",
    "        temp_no_CH4_data = generate_df_with_noise_std(\n",
    "            df=CH4_data_clean, n_repeat=500, noise_std=0\n",
    "        )\n",
    "        temp_no_airless_data = generate_df_with_noise_std(\n",
    "            df=airless_data_clean, n_repeat=5000, noise_std=0\n",
    "        )\n",
    "\n",
    "        current_clean_df = pd.concat(\n",
    "            [temp_no_CO2_data, temp_no_CH4_data, temp_no_airless_data],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "        normalized_data = normalize_min_max_by_row(current_clean_df.iloc[:, -n_points:])\n",
    "        list_of_clean_arrays.append(normalized_data.values.astype(\"float32\"))\n",
    "\n",
    "        del (\n",
    "            temp_no_CO2_data,\n",
    "            temp_no_CH4_data,\n",
    "            temp_no_airless_data,\n",
    "            current_clean_df,\n",
    "            normalized_data,\n",
    "        )\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"\\n--- Final Data Concatenation ---\")\n",
    "    X_noisy = np.concatenate(list_of_noisy_arrays, axis=0)\n",
    "    del list_of_noisy_arrays\n",
    "\n",
    "    X_no_noisy = np.concatenate(list_of_clean_arrays, axis=0)\n",
    "    del list_of_clean_arrays\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    print(\"\\n--- Saving generated data to disk for future runs ---\")\n",
    "    np.save(noisy_path, X_noisy)\n",
    "    np.save(clean_path, X_no_noisy)\n",
    "    print(f\"Data saved to directory: '{output_dir}'\")\n",
    "\n",
    "    print(f\"\\nFinal noisy data shape: {X_noisy.shape}\")\n",
    "    print(f\"Final clean data shape: {X_no_noisy.shape}\")\n",
    "    assert (\n",
    "        X_noisy.shape[0] == X_no_noisy.shape[0]\n",
    "    ), \"The number of samples does not match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FInal data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data prep and Model Training\n",
    "\n",
    "The rest of your script remains the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 18:15:08.702022: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-09 18:15:08.808237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754752508.844273    4578 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754752508.855430    4578 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754752508.924072    4578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754752508.924101    4578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754752508.924103    4578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754752508.924104    4578 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-09 18:15:08.934436: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set memory growth for 1 GPU(s)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Set memory growth to True for all GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f\"Set memory growth for {len(gpus)} GPU(s)\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "\n",
    "X_train_noisy, X_test_noisy, X_train_clean, X_test_clean = train_test_split(\n",
    "    X_noisy, X_no_noisy, test_size=test_size, random_state=42\n",
    ")\n",
    "del (X_noisy, X_no_noisy)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754722660.823664   19010 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1818 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow datasets created on CPU and NumPy arrays removed from RAM.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your batch size from the fit command\n",
    "batch_size = 64 \n",
    "\n",
    "with tf.device('/CPU:0'):\n",
    "    # Create a training dataset from your numpy arrays\n",
    "    tf_train_dataset = tf.data.Dataset.from_tensor_slices((X_train_noisy, X_train_clean))\n",
    "    \n",
    "    # Create a validation dataset\n",
    "    tf_val_dataset = tf.data.Dataset.from_tensor_slices((X_test_noisy, X_test_clean))\n",
    "\n",
    "# Now, configure the datasets for performance (shuffling, batching, etc.)\n",
    "tf_train_dataset = tf_train_dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "tf_val_dataset = tf_val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# IMPORTANT: Clean up the huge numpy arrays to free RAM\n",
    "del X_train_noisy, X_train_clean, X_test_noisy, X_test_clean\n",
    "gc.collect()\n",
    "\n",
    "print(\"TensorFlow datasets created on CPU and NumPy arrays removed from RAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">385</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,632</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">153,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">154,112</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">262,656</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">385</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">197,505</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m385\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m197,632\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m153,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │        \u001b[38;5;34m90,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │        \u001b[38;5;34m90,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │        \u001b[38;5;34m90,300\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m154,112\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m262,656\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m385\u001b[0m)            │       \u001b[38;5;34m197,505\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,024,673</span> (7.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,024,673\u001b[0m (7.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,024,673</span> (7.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,024,673\u001b[0m (7.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# input_dim = X_train_noisy.shape[1]\n",
    "input_dim = tf_train_dataset.element_spec[0].shape[1]\n",
    "\n",
    "input_spectrum = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoded = layers.Dense(512, activation=\"swish\")(input_spectrum)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(512, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(512, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(300, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(input_dim, activation=\"linear\")(decoded)\n",
    "\n",
    "autoencoder = keras.Model(inputs=input_spectrum, outputs=decoded)\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "autoencoder.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1754722693.410582   26495 service.cc:152] XLA service 0x7f2b58016000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1754722693.411639   26495 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2025-08-09 09:58:13.545544: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1754722694.187711   26495 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-08-09 09:58:16.858866: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4124', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "2025-08-09 09:58:17.054398: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4124', 8300 bytes spill stores, 8520 bytes spill loads\n",
      "\n",
      "2025-08-09 09:58:17.832002: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4124', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "2025-08-09 09:58:17.861181: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4124', 988 bytes spill stores, 1052 bytes spill loads\n",
      "\n",
      "2025-08-09 09:58:18.603926: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4126', 268 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "2025-08-09 09:58:18.604943: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4126', 280 bytes spill stores, 280 bytes spill loads\n",
      "\n",
      "2025-08-09 09:58:19.762602: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_4126', 320 bytes spill stores, 320 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   55/20000\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 3ms/step - loss: 0.1860  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754722702.511398   26495 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 5ms/step - loss: 0.0959 - val_loss: 0.0525\n",
      "Epoch 2/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3ms/step - loss: 0.0482 - val_loss: 0.0160\n",
      "Epoch 3/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 6ms/step - loss: 0.0250 - val_loss: 0.0140\n",
      "Epoch 4/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 4ms/step - loss: 0.0215 - val_loss: 0.0113\n",
      "Epoch 5/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0197 - val_loss: 0.0102\n",
      "Epoch 6/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2ms/step - loss: 0.0187 - val_loss: 0.0099\n",
      "Epoch 7/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - loss: 0.0178 - val_loss: 0.0093\n",
      "Epoch 8/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0171 - val_loss: 0.0093\n",
      "Epoch 9/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step - loss: 0.0164 - val_loss: 0.0081\n",
      "Epoch 10/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 3ms/step - loss: 0.0156 - val_loss: 0.0081\n",
      "Epoch 11/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - loss: 0.0150 - val_loss: 0.0076\n",
      "Epoch 12/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2ms/step - loss: 0.0144 - val_loss: 0.0070\n",
      "Epoch 13/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step - loss: 0.0139 - val_loss: 0.0079\n",
      "Epoch 14/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0134 - val_loss: 0.0069\n",
      "Epoch 15/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 0.0129 - val_loss: 0.0073\n",
      "Epoch 16/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2ms/step - loss: 0.0124 - val_loss: 0.0066\n",
      "Epoch 17/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2ms/step - loss: 0.0120 - val_loss: 0.0064\n",
      "Epoch 18/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0115 - val_loss: 0.0064\n",
      "Epoch 19/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step - loss: 0.0112 - val_loss: 0.0066\n",
      "Epoch 20/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0108 - val_loss: 0.0060\n",
      "Epoch 21/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0105 - val_loss: 0.0067\n",
      "Epoch 22/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0103 - val_loss: 0.0066\n",
      "Epoch 23/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0101 - val_loss: 0.0061\n",
      "Epoch 24/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3ms/step - loss: 0.0099 - val_loss: 0.0058\n",
      "Epoch 25/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3ms/step - loss: 0.0097 - val_loss: 0.0056\n",
      "Epoch 26/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2ms/step - loss: 0.0095 - val_loss: 0.0063\n",
      "Epoch 27/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2ms/step - loss: 0.0093 - val_loss: 0.0059\n",
      "Epoch 28/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 0.0092 - val_loss: 0.0059\n",
      "Epoch 29/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 0.0091 - val_loss: 0.0052\n",
      "Epoch 30/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step - loss: 0.0090 - val_loss: 0.0057\n",
      "Epoch 31/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step - loss: 0.0089 - val_loss: 0.0053\n",
      "Epoch 32/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2ms/step - loss: 0.0089 - val_loss: 0.0057\n",
      "Epoch 33/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - loss: 0.0088 - val_loss: 0.0053\n",
      "Epoch 34/100\n",
      "\u001b[1m20000/20000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 3ms/step - loss: 0.0087 - val_loss: 0.0059\n"
     ]
    }
   ],
   "source": [
    "history = autoencoder.fit(\n",
    "    tf_train_dataset,  # Pass the training dataset\n",
    "    epochs=100,\n",
    "    validation_data=tf_val_dataset,  # Pass the validation dataset\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "autoencoder.save(\"Models/AE_CH4.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1754752541.758503    4578 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4138 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data for final evaluation...\n",
      "Test data re-loaded successfully.\n",
      "\n",
      "Plotting training history...\n",
      "Training history not available to plot.\n",
      "\n",
      "Predicting on test data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Predict reconstructed spectra on test data\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPredicting on test data...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m decoded_spectra = \u001b[43mautoencoder\u001b[49m.predict(X_test_noisy, batch_size=BATCH_SIZE)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Visualize a few reconstructions\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mVisualizing sample reconstructions...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "\n",
    "saved_autoencoder = keras.models.load_model(\"Models/AE_CH4.keras\")\n",
    "\n",
    "\n",
    "# Define constants again to make this cell self-contained\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"Loading test data for final evaluation...\")\n",
    "X_noisy_full = np.load(noisy_path)\n",
    "X_clean_full = np.load(clean_path)\n",
    "\n",
    "# Perform the exact same train-test split to get the identical test set\n",
    "# Using the same RANDOM_STATE is crucial here.\n",
    "_, X_test_noisy, _, X_test_clean = train_test_split(\n",
    "    X_noisy_full, X_clean_full, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# We can now delete the full arrays again as we only need the test set\n",
    "del X_noisy_full, X_clean_full\n",
    "gc.collect()\n",
    "print(\"Test data re-loaded successfully.\")\n",
    "\n",
    "# Plot training and validation MAE\n",
    "# This assumes 'history' object is still available from the training cell\n",
    "print(\"\\nPlotting training history...\")\n",
    "if 'history' in locals():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history[\"loss\"], label=\"Training MAE\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"Validation MAE\")\n",
    "    plt.title(\"Model MAE Progress During Training\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MAE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Training history not available to plot.\")\n",
    "\n",
    "\n",
    "# Predict reconstructed spectra on test data\n",
    "print(\"\\nPredicting on test data...\")\n",
    "decoded_spectra = autoencoder.predict(X_test_noisy, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "# Visualize a few reconstructions\n",
    "print(\"Visualizing sample reconstructions...\")\n",
    "num_samples = 5  # Number of samples to visualize\n",
    "indices = np.random.choice(len(X_test_noisy), num_samples, replace=False)\n",
    "\n",
    "for idx in indices:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(waves, X_test_clean[idx].flatten(), label=\"Original Clean Spectrum\", color='blue')\n",
    "    plt.plot(waves, X_test_noisy[idx].flatten(), label=\"Noisy Input Spectrum\", color='gray', alpha=0.6)\n",
    "    plt.plot(\n",
    "        waves,\n",
    "        decoded_spectra[idx].flatten(),\n",
    "        label=\"Denoised (Reconstructed) Spectrum\",\n",
    "        linestyle=\"--\",\n",
    "        color='red'\n",
    "    )\n",
    "    plt.xlabel(\"Wavelength\")\n",
    "    plt.ylabel(\"Normalized Intensity\")\n",
    "    plt.title(f\"Spectrum Reconstruction - Sample {idx}\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### Final Performance Metrics\n",
    "\n",
    "# %%\n",
    "print(\"\\nCalculating final performance metrics...\")\n",
    "mae = mean_absolute_error(X_test_clean, decoded_spectra)\n",
    "print(f\"Final Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "\n",
    "mse = mean_squared_error(X_test_clean, decoded_spectra)\n",
    "print(f\"Final Mean Squared Error (MSE): {mse:.6f}\")\n",
    "\n",
    "# Flatten for R² score\n",
    "r2 = r2_score(X_test_clean.flatten(), decoded_spectra.flatten())\n",
    "print(f\"Final Coefficient of Determination (R²): {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2ms/step\n",
      "Mean Absolute Error (MAE): 0.006079\n",
      "Mean Squared Error (MSE): 0.000954\n",
      "Coefficient of Determination (R²): 0.982316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Re-load the best model saved by EarlyStopping\n",
    "autoencoder = keras.models.load_model(\"Models/AE_CH4_OG.keras\")\n",
    "X_reconstructed = autoencoder.predict(X_test_noisy)\n",
    "\n",
    "# Compute evaluation metrics on the test set\n",
    "mae = mean_absolute_error(X_test_clean, X_reconstructed)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "\n",
    "mse = mean_squared_error(X_test_clean, X_reconstructed)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.6f}\")\n",
    "\n",
    "r2 = r2_score(X_test_clean.flatten(), X_reconstructed.flatten())\n",
    "print(f\"Coefficient of Determination (R²): {r2:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate spectra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiREx Autoencoder for Exoplanet Spectra Denoising (All Materials)\n",
    "\n",
    "This script trains a single, robust deep learning model to remove noise and stellar\n",
    "contamination from a comprehensive set of exoplanet transit spectra, including various\n",
    "biosignatures (CH4, H2O, O3), CO2, and airless scenarios.\n",
    "\n",
    "This version is fully optimized for memory efficiency on both CPU and GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multirex as mrex\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import numpy as np\n",
    "# import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import warnings\n",
    "# import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow memory growth set for 1 GPU(s).\n"
     ]
    }
   ],
   "source": [
    "def remove_warnings():\n",
    "    \"\"\"Suppresses specified warnings for cleaner output.\"\"\"\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    warnings.filterwarnings(\n",
    "        \"ignore\",\n",
    "        category=UserWarning,\n",
    "        message=\"Pandas doesn't allow columns to be created via a new attribute name*\",\n",
    "    )\n",
    "    warnings.filterwarnings(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "def configure_gpu_memory_growth():\n",
    "    \"\"\"Prevents TensorFlow from allocating all GPU memory at once.\"\"\"\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "      try:\n",
    "        for gpu in gpus:\n",
    "          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"TensorFlow memory growth set for {len(gpus)} GPU(s).\")\n",
    "      except RuntimeError as e:\n",
    "        print(f\"GPU memory growth could not be set: {e}\")\n",
    "\n",
    "# Initial setup\n",
    "remove_warnings()\n",
    "configure_gpu_memory_growth()\n",
    "\n",
    "# Load and prepare wavelength data\n",
    "waves = np.loadtxt(\"waves.txt\")\n",
    "n_points = len(waves)\n",
    "wn_grid = np.sort((10000 / waves))\n",
    "\n",
    "\n",
    "def apply_contaminations_from_files(contamination_files, df, n_points):\n",
    "    \"\"\"\n",
    "    Applies multiple stellar contaminations to the spectral data.\n",
    "    This version is optimized to prevent DataFrame fragmentation.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    # Add non-contaminated case\n",
    "    df_no_contam = df.copy().assign(f_spot=0.0, f_fac=0.0)\n",
    "    cols = [\"f_spot\", \"f_fac\"] + [col for col in df.columns if col not in [\"f_spot\", \"f_fac\"]]\n",
    "    df_list.append(df_no_contam[cols])\n",
    "    \n",
    "    pattern = r\"fspot(?P<f_spot>[0-9.]+)_ffac(?P<f_fac>[0-9.]+)\\.txt$\"\n",
    "\n",
    "    for file_path in contamination_files:\n",
    "        if not os.path.isfile(file_path): raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        \n",
    "        filename = os.path.basename(file_path)\n",
    "        match = re.search(pattern, filename)\n",
    "        if not match: raise ValueError(f\"Filename '{filename}' does not match pattern.\")\n",
    "\n",
    "        f_spot = float(match.group(\"f_spot\"))\n",
    "        f_fac = float(match.group(\"f_fac\"))\n",
    "\n",
    "        try:\n",
    "            contam_data = np.loadtxt(file_path, ndmin=2)\n",
    "            contam_values = contam_data[:, 1] if contam_data.shape[1] >= 2 else contam_data.flatten()\n",
    "            if len(contam_values) != n_points: raise ValueError(f\"Contamination values in '{filename}' != n_points.\")\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "        df_contam = df.copy()\n",
    "        data_columns = df_contam.columns[-n_points:]\n",
    "        df_contam[data_columns] *= contam_values[::-1]\n",
    "        df_contam = df_contam.assign(f_spot=f_spot, f_fac=f_fac)\n",
    "        df_list.append(df_contam[cols])\n",
    "\n",
    "    df_final = pd.concat(df_list, ignore_index=True).copy()\n",
    "    df_final.data = df_final.iloc[:, -n_points:]\n",
    "    df_final.params = df_final.iloc[:, :-n_points]\n",
    "    return df_final\n",
    "\n",
    "def filter_rows(df):\n",
    "    \"\"\"Filters rows based on atmospheric composition values.\"\"\"\n",
    "    filter_cols = [\"atm CH4\", \"atm O3\", \"atm H2O\"]\n",
    "    for chem in [col for col in filter_cols if col in df.columns]:\n",
    "        df = df[df[chem] >= -8].copy()\n",
    "    df.data = df.iloc[:, -n_points:]\n",
    "    df.params = df.iloc[:, :-n_points]\n",
    "    return df\n",
    "\n",
    "def load_and_prep_data(filepath, n_points):\n",
    "    \"\"\"Reads a CSV and sets appropriate data types for memory efficiency.\"\"\"\n",
    "    df = pd.read_csv(filepath)\n",
    "    df[df.columns[-n_points:]] = df[df.columns[-n_points:]].astype('float32')\n",
    "    return df\n",
    "\n",
    "def normalize_min_max_by_row(df):\n",
    "    \"\"\"Normalizes each row of a DataFrame to a [0, 1] range.\"\"\"\n",
    "    min_vals = df.min(axis=1)\n",
    "    max_vals = df.max(axis=1)\n",
    "    range_vals = max_vals - min_vals\n",
    "    range_vals[range_vals == 0] = 1  # Avoid division by zero\n",
    "    return (df.sub(min_vals, axis=0)).div(range_vals, axis=0)\n",
    "\n",
    "def generate_df_with_noise_std(df, n_repeat, noise_std, seed=None):\n",
    "    \"\"\"Generates a new DataFrame by applying Gaussian noise to the spectra.\"\"\"\n",
    "    df_params = getattr(df, 'params', pd.DataFrame())\n",
    "    df_spectra = getattr(df, 'data', df).astype('float32')\n",
    "    \n",
    "    if seed is not None: np.random.seed(seed)\n",
    "    \n",
    "    replicated_spectra_vals = np.repeat(df_spectra.values, n_repeat, axis=0)\n",
    "    \n",
    "    if isinstance(noise_std, (int, float)):\n",
    "        noise = np.random.normal(0, noise_std, replicated_spectra_vals.shape).astype('float32')\n",
    "    else:\n",
    "        noise_array = np.array(noise_std, dtype='float32')\n",
    "        noise_replicated = np.tile(np.repeat(noise_array[:, np.newaxis], n_repeat, axis=0), (1, df_spectra.shape[1]))\n",
    "        noise = np.random.normal(0, noise_replicated, replicated_spectra_vals.shape).astype('float32')\n",
    "        \n",
    "    noisy_spectra = pd.DataFrame(replicated_spectra_vals + noise, columns=df_spectra.columns)\n",
    "\n",
    "    replicated_params = pd.DataFrame(np.repeat(df_params.values, n_repeat, axis=0), columns=df_params.columns)\n",
    "    \n",
    "    new_cols_data = {\"noise_std\": np.repeat(noise_std, n_repeat * len(df_params)) if isinstance(noise_std, (list, np.ndarray)) else noise_std, \"n_repeat\": n_repeat}\n",
    "    new_cols_df = pd.DataFrame(new_cols_data)\n",
    "\n",
    "    df_final = pd.concat([new_cols_df, replicated_params.reset_index(drop=True), noisy_spectra.reset_index(drop=True)], axis=1)\n",
    "    df_final.data = df_final.iloc[:, -n_points:]\n",
    "    df_final.params = df_final.iloc[:, :-n_points]\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process All Source Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing source datasets...\n",
      "Error during data loading: 'list' object has no attribute 'assign'\n"
     ]
    }
   ],
   "source": [
    "contamination_files = [\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.01_ffac0.70.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.08_ffac0.70.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.08.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.54.txt\",\n",
    "    \"stellar_contamination/TRAPPIST-1_contam_fspot0.26_ffac0.70.txt\",\n",
    "]\n",
    "\n",
    "data_sources = {}\n",
    "try:\n",
    "    print(\"Loading and preparing source datasets...\")\n",
    "    # Load all base datasets\n",
    "    data_sources['airless'] = load_and_prep_data(\"spec_data/airless_data.csv\", n_points)\n",
    "    data_sources['CO2'] = load_and_prep_data(\"spec_data/CO2_data.csv\", n_points)\n",
    "    data_sources['CH4'] = filter_rows(load_and_prep_data(\"spec_data/CH4_data.csv\", n_points))\n",
    "    data_sources['O3'] = filter_rows(load_and_prep_data(\"spec_data/O3_data.csv\", n_points))\n",
    "    data_sources['H2O'] = filter_rows(load_and_prep_data(\"spec_data/H2O_data.csv\", n_points))\n",
    "    data_sources['CH4_O3'] = filter_rows(load_and_prep_data(\"spec_data/CH4_O3_data.csv\", n_points))\n",
    "    data_sources['CH4_H2O'] = filter_rows(load_and_prep_data(\"spec_data/CH4_H2O_data.csv\", n_points))\n",
    "    data_sources['O3_H2O'] = filter_rows(load_and_prep_data(\"spec_data/O3_H2O_data.csv\", n_points))\n",
    "    data_sources['CH4_O3_H2O'] = filter_rows(load_and_prep_data(\"spec_data/CH4_O3_H2O_data.csv\", n_points))\n",
    "\n",
    "    # Prepare two lists: one for clean data, one for contaminated data\n",
    "    clean_data_list = list(data_sources.values())\n",
    "    contaminated_data_list = [apply_contaminations_from_files(d, contamination_files, n_points) for d in clean_data_list]\n",
    "    \n",
    "    print(\"...Data loading complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during data loading: {e}\")\n",
    "    \n",
    "# Define the number of repetitions for each dataset type per SNR level\n",
    "# These values are taken from the original unoptimized notebook.\n",
    "n_repeats_per_snr = {\n",
    "    1: [2000, 2000, 200, 200, 200, 20, 20, 20, 4],\n",
    "    3: [2000, 2000, 200, 200, 200, 20, 20, 20, 4],\n",
    "    6: [1500, 1500, 150, 150, 150, 15, 15, 15, 3],\n",
    "    10: [1000, 1000, 100, 100, 100, 10, 10, 10, 2],\n",
    "    None: [4000, 3000, 300, 300, 300, 30, 30, 30, 5]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Full Combined Dataset with Caching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'contaminated_data_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m...Loading complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# --- FIX: Check if data loading was successful before proceeding ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontaminated_data_list\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m clean_data_list:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mData loading failed in the previous cell. Please check file paths and try again.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCached data not found. Generating full combined dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'contaminated_data_list' is not defined"
     ]
    }
   ],
   "source": [
    "output_dir = \"processed_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Use general filenames for the combined dataset\n",
    "noisy_path = os.path.join(output_dir, \"ALL_X_noisy_full_dataset.npy\")\n",
    "clean_path = os.path.join(output_dir, \"ALL_X_clean_full_dataset.npy\")\n",
    "\n",
    "if os.path.exists(noisy_path) and os.path.exists(clean_path):\n",
    "    print(\"Found cached combined data. Loading from disk...\")\n",
    "    X_noisy = np.load(noisy_path)\n",
    "    X_no_noisy = np.load(clean_path)\n",
    "    print(\"...Loading complete.\")\n",
    "else:\n",
    "    # --- FIX: Check if data loading was successful before proceeding ---\n",
    "    if not contaminated_data_list or not clean_data_list:\n",
    "        raise ValueError(\"Data loading failed in the previous cell. Please check file paths and try again.\")\n",
    "        \n",
    "    print(\"Cached data not found. Generating full combined dataset...\")\n",
    "    \n",
    "    list_of_noisy_arrays, list_of_clean_arrays = [], []\n",
    "    snr_values = [1, 3, 6, 10, None] \n",
    "\n",
    "    for snr in snr_values:\n",
    "        print(f\"--- Processing SNR = {snr if snr is not None else 'inf'} ---\")\n",
    "        # Use the contaminated CO2 data (index 1) as a consistent reference for noise calculation\n",
    "        noise = mrex.generate_df_SNR_noise(df=contaminated_data_list[1], n_repeat=1, SNR=snr)[\"noise\"][0] if snr is not None else 0.0\n",
    "        \n",
    "        n_repeats = n_repeats_per_snr[snr]\n",
    "        temp_noisy_dfs, temp_clean_dfs = [], []\n",
    "\n",
    "        # Iterate through all data types and apply noise with correct repetitions\n",
    "        for i, (contam_df, clean_df) in enumerate(zip(contaminated_data_list, clean_data_list)):\n",
    "            temp_noisy_dfs.append(generate_df_with_noise_std(df=contam_df, n_repeat=n_repeats[i], noise_std=noise))\n",
    "            temp_clean_dfs.append(generate_df_with_noise_std(df=clean_df, n_repeat=n_repeats[i], noise_std=0))\n",
    "\n",
    "        # Concatenate all data for this SNR level\n",
    "        noisy_df = pd.concat(temp_noisy_dfs, ignore_index=True)\n",
    "        clean_df = pd.concat(temp_clean_dfs, ignore_index=True)\n",
    "        \n",
    "        # Append normalized numpy arrays to the main list\n",
    "        list_of_noisy_arrays.append(normalize_min_max_by_row(noisy_df.iloc[:, -n_points:]).values.astype(\"float32\"))\n",
    "        list_of_clean_arrays.append(normalize_min_max_by_row(clean_df.iloc[:, -n_points:]).values.astype(\"float32\"))\n",
    "        \n",
    "        # Clean up memory for this iteration\n",
    "        del temp_noisy_dfs, temp_clean_dfs, noisy_df, clean_df\n",
    "        gc.collect()\n",
    "\n",
    "    # Final concatenation of numpy arrays from all SNR levels\n",
    "    X_noisy = np.concatenate(list_of_noisy_arrays, axis=0)\n",
    "    X_no_noisy = np.concatenate(list_of_clean_arrays, axis=0)\n",
    "    del list_of_noisy_arrays, list_of_clean_arrays\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n--- Saving combined data to disk for future runs ---\")\n",
    "    np.save(noisy_path, X_noisy)\n",
    "    np.save(clean_path, X_no_noisy)\n",
    "\n",
    "print(f\"\\nFinal noisy data shape: {X_noisy.shape}\")\n",
    "print(f\"Final clean data shape: {X_no_noisy.shape}\")\n",
    "assert X_noisy.shape[0] == X_no_noisy.shape[0], \"Sample count mismatch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build and Train Autoencoder Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Optimized TensorFlow Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_noisy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Create datasets on CPU to prevent VRAM overflow on initialization\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.device(\u001b[33m'\u001b[39m\u001b[33m/CPU:0\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      7\u001b[39m     X_train_noisy, X_test_noisy, X_train_clean, X_test_clean = train_test_split(\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         \u001b[43mX_noisy\u001b[49m, X_no_noisy, test_size=TEST_SIZE, random_state=RANDOM_STATE\n\u001b[32m      9\u001b[39m     )\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# Create efficient TensorFlow dataset pipelines\u001b[39;00m\n\u001b[32m     12\u001b[39m     train_dataset = tf.data.Dataset.from_tensor_slices((X_train_noisy, X_train_clean))\n",
      "\u001b[31mNameError\u001b[39m: name 'X_noisy' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Create datasets on CPU to prevent VRAM overflow on initialization\n",
    "with tf.device('/CPU:0'):\n",
    "    X_train_noisy, X_test_noisy, X_train_clean, X_test_clean = train_test_split(\n",
    "        X_noisy, X_no_noisy, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    # Create efficient TensorFlow dataset pipelines\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_noisy, X_train_clean))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((X_test_noisy, X_test_clean))\n",
    "    validation_dataset = validation_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Clean up large NumPy arrays from RAM immediately after creating TF datasets\n",
    "del X_noisy, X_no_noisy, X_train_noisy, X_train_clean\n",
    "gc.collect()\n",
    "\n",
    "print(\"TensorFlow datasets created and source NumPy arrays cleared from RAM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and Compile the Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer input dimension directly from the dataset specification\n",
    "input_dim = validation_dataset.element_spec[0].shape[1]\n",
    "\n",
    "input_spectrum = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# Encoder\n",
    "encoded = layers.Dense(512, activation=\"swish\")(input_spectrum)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(512, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(512, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "encoded = layers.Dropout(0.2)(encoded)\n",
    "encoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "\n",
    "# Decoder\n",
    "decoded = layers.Dense(300, activation=\"swish\")(encoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(512, activation=\"swish\")(decoded)\n",
    "decoded = layers.Dropout(0.2)(decoded)\n",
    "decoded = layers.Dense(input_dim, activation=\"linear\")(decoded)\n",
    "\n",
    "autoencoder = keras.Model(inputs=input_spectrum, outputs=decoded)\n",
    "optimizer = Adam(learning_rate=0.00001)\n",
    "autoencoder.compile(optimizer=optimizer, loss=\"mae\")\n",
    "\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Save the general model\n",
    "autoencoder.save(\"AE_ALL_MATERIALS.keras\")\n",
    "print(\"Model training complete and saved to AE_ALL_MATERIALS.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate and Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotting training history...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history[\"loss\"], label=\"Training MAE\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation MAE\")\n",
    "plt.title(\"Model MAE Progress During Training\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Mean Absolute Error (MAE)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEvaluating model on test data...\")\n",
    "# Reload test data from cache for evaluation\n",
    "print(\"Loading test data for evaluation...\")\n",
    "noisy_path = os.path.join(output_dir, \"ALL_X_noisy_full_dataset.npy\")\n",
    "clean_path = os.path.join(output_dir, \"ALL_X_clean_full_dataset.npy\")\n",
    "\n",
    "X_noisy_full = np.load(noisy_path)\n",
    "X_clean_full = np.load(clean_path)\n",
    "_, X_test_noisy_eval, _, X_test_clean_eval = train_test_split(\n",
    "    X_noisy_full, X_clean_full, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
    ")\n",
    "del X_noisy_full, X_clean_full\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"Predicting on test data...\")\n",
    "decoded_spectra = autoencoder.predict(X_test_noisy_eval, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Visualize a few reconstructions\n",
    "print(\"Visualizing sample reconstructions...\")\n",
    "num_samples = 5\n",
    "indices = np.random.choice(len(X_test_noisy_eval), num_samples, replace=False)\n",
    "\n",
    "for idx in indices:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(waves, X_test_clean_eval[idx].flatten(), label=\"Original Clean Spectrum\", color='blue')\n",
    "    plt.plot(waves, X_test_noisy_eval[idx].flatten(), label=\"Noisy Input Spectrum\", color='gray', alpha=0.6)\n",
    "    plt.plot(waves, decoded_spectra[idx].flatten(), label=\"Denoised Spectrum\", color='red', linestyle='--')\n",
    "    plt.xlabel(\"Wavelength\")\n",
    "    plt.ylabel(\"Normalized Intensity\")\n",
    "    plt.title(f\"Spectrum Reconstruction - Sample {idx}\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Performance Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCalculating final performance metrics...\")\n",
    "mae = mean_absolute_error(X_test_clean_eval, decoded_spectra)\n",
    "print(f\"Final Mean Absolute Error (MAE): {mae:.6f}\")\n",
    "\n",
    "mse = mean_squared_error(X_test_clean_eval, decoded_spectra)\n",
    "print(f\"Final Mean Squared Error (MSE): {mse:.6f}\")\n",
    "\n",
    "# Flatten for R² score\n",
    "r2 = r2_score(X_test_clean_eval.flatten(), decoded_spectra.flatten())\n",
    "print(f\"Final Coefficient of Determination (R²): {r2:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
